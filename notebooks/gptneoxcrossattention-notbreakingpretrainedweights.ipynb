{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor,  AutoTokenizer, AutoModelForCausalLM, ViTModel,  BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel, VisionEncoderDecoderModel ,ViTImageProcessor\n",
    "\n",
    "from transformers import AutoImageProcessor,  AutoTokenizer, AutoModelForCausalLM, ViTModel,  BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel, VisionEncoderDecoderModel ,ViTImageProcessor\n",
    "\n",
    "\n",
    "from typing import * \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import io, transforms\n",
    "import torchvision.datasets as dset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import EncoderDecoderModel, GPT2Tokenizer, ViTFeatureExtractor\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1626e998",
   "metadata": {},
   "source": [
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"NinedayWang/PolyCoder-0.4B\")\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NinedayWang/PolyCoder-0.4B\")\n",
    "\n",
    "\n",
    "encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "encoder_config = encoder.config\n",
    "encoder_config.add_cross_attention = True\n",
    "encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\",config = encoder_config)\n",
    "\n",
    "decoder = AutoModelForCausalLM.from_pretrained(\"NinedayWang/PolyCoder-0.4B\")\n",
    "\n",
    "decoder_config = decoder.config\n",
    "decoder_config.add_cross_attention = True\n",
    "\n",
    "decoder = AutoModelForCausalLM.from_pretrained(\"NinedayWang/PolyCoder-0.4B\", config = decoder_config)\n",
    "\n",
    "\n",
    "#import new \n",
    "decoder.forward = types.MethodType(decoder, forward)\n",
    "\n",
    "\"\"\"\n",
    "decoder.q = new.instancemethod(forward, decoder, None)\n",
    "z is z.q()  # true\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Set encoder config hidden size (which will make sure no projection layer is added)\n",
    "setattr(encoder.config, \"is_cross_attention\", True)\n",
    "\"\"\"\n",
    "# Initializing a model with a pretrained Swin as encoder & a pretrained BART-large as decoder\n",
    "model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c13687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor,  AutoTokenizer, AutoModelForCausalLM, ViTModel,  BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel, VisionEncoderDecoderModel ,ViTImageProcessor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d22db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "  \"_name_or_path\": \"NinedayWang/PolyCoder-0.4B\",\n",
    "  \"add_cross_attention\": True,\n",
    "  \"architectures\": [\n",
    "    \"GPTNeoXForCausalLM\"\n",
    "  ],\n",
    "  \"bos_token_id\": 0,\n",
    "  \"eos_token_id\": 0,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_size\": 1024,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 4096,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 2048,\n",
    "  \"model_type\": \"gpt_neox\",\n",
    "  \"num_attention_heads\": 16,\n",
    "  \"num_hidden_layers\": 24,\n",
    "  \"rotary_emb_base\": 10000,\n",
    "  \"rotary_pct\": 1.0,\n",
    "  \"tie_word_embeddings\": False,\n",
    "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
    "    \"is_cross_attention\":True,\n",
    "  \"torch_dtype\": \"float32\",\n",
    "  \"transformers_version\": \"4.27.3\",\n",
    "  \"use_cache\": True,\n",
    "  \"use_parallel_residual\": False,\n",
    "  \"vocab_size\": 50304\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DesignCoder.model import  GPTNeoXCrossAttentionForCausalLM\n",
    "from DesignCoder.model_with_new_attention import GPTNeoXEmbedAttentionForCausalLM\n",
    "from codelms.DesignCoder.model import GPTNeoXForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58734e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce137d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTNeoXForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from packaging import version\n",
    "assert version.parse(transformers.__version__) >= version.parse(\"4.23.0\")\n",
    "\n",
    "\n",
    "\"\"\"model = GPTNeoXForCausalLM.from_pretrained(\"Code-LMs/Convert2HF/polycoder/0-4B/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NinedayWang/PolyCoder-2.7B\")\n",
    "\"\"\"\n",
    "\n",
    "model_path = \"/Users/mehmetburaksayici/Desktop/sahibinden/ccrraawwll/_designcoder/\"\n",
    "tokenizer_path = \"/Users/mehmetburaksayici/Desktop/sahibinden/ccrraawwll/_designcoder_tokenizer/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "decoder = GPTNeoXEmbedAttentionForCausalLM.from_pretrained(model_path) # GPTNeoXCrossAttentionForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884ef45",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "prompt = '''def binarySearch(arr, left, right, x):\n",
    "    mid = (left +'''\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "result = decoder.generate(input_ids, max_length=50, num_beams=1, num_return_sequences=1)\n",
    "for res in result:\n",
    "    print(tokenizer.decode(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e95f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "107520/192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18f545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "105*16*192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"NinedayWang/PolyCoder-0.4B\")\n",
    "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NinedayWang/PolyCoder-0.4B\")\n",
    "\n",
    "\n",
    "encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "encoder_config = encoder.config\n",
    "encoder_config.add_cross_attention = True\n",
    "encoder = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\",config = encoder_config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initializing a model with a pretrained Swin as encoder & a pretrained BART-large as decoder\n",
    "model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b972a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7883fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MEAN = (0.485, 0.456, 0.406)\n",
    "STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "TRAIN_PCT = 0.95\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 3\n",
    "LR = 1e-4\n",
    "IMAGE_SIZE = (224, 224)\n",
    "\n",
    "MAX_TEXT_LENGTH = 32\n",
    "\n",
    "LABEL_MASK = -100\n",
    "\n",
    "TOP_K = 1000\n",
    "TOP_P = 0.95\n",
    "\n",
    "\n",
    "tfms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=0.5, \n",
    "            std=0.5\n",
    "        )\n",
    "   ]\n",
    ")\n",
    "descale = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(\n",
    "            mean = [ 0., 0., 0. ],\n",
    "            std = 1 / 0.5\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean = -0.5,\n",
    "            std = [ 1., 1., 1. ]\n",
    "        ),                           \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "images =  Image.fromarray(np.zeros((224,224,3)).astype(np.uint8))\n",
    "\n",
    "pixel_values = image_processor(images).pixel_values\n",
    "inp_ = torch.from_numpy(pixel_values[0])[None,]\n",
    "\"\"\"encoder_outputs = encoder(inp_)\n",
    "encoder_outputs.keys()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e1bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_ = torch.from_numpy(pixel_values[0])[None,]\n",
    "model.generate(inp_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891499a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.gpt_neox.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54865801",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(torch.zeros(1,197,1024),torch.zeros(1,1,1024),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.zeros(1,197,1024)*torch.zeros(1,1,1024)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c448b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7dd8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d6d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed6215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76807af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286f826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f332620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65642b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "def generate_sentence_from_image(model, encoder_outputs, tokenizer, max_text_length: int, device)-> List[str]:\n",
    "    generated_so_far = torch.LongTensor([[tokenizer.bos_token_id]]*len(encoder_outputs.last_hidden_state)).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(max_text_length)):\n",
    "            attention_mask = torch.ones_like(generated_so_far)\n",
    "            decoder_out = model(\n",
    "                decoder_input_ids=generated_so_far, \n",
    "                decoder_attention_mask=attention_mask,\n",
    "                encoder_outputs=encoder_outputs\n",
    "            )\n",
    "\n",
    "            next_token_logits = decoder_out[\"logits\"][:, -1, :]\n",
    "            filtered_p = top_k_top_p_filtering(next_token_logits, top_k=TOP_K, top_p=TOP_P, device=device)\n",
    "            next_token = torch.multinomial(filtered_p, num_samples=1)\n",
    "            generated_so_far = torch.cat((generated_so_far, next_token), dim=1)\n",
    "\n",
    "    return [tokenizer.decode(coded_sentence) for coded_sentence in generated_so_far]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "images =  Image.fromarray(np.zeros((224,224,3)).astype(np.uint8))\n",
    "\n",
    "pixel_values = image_processor(images).pixel_values\n",
    "\n",
    "encoder_outputs = model.encoder(pixel_values=tfms(images)[None,:])\n",
    "\n",
    "\n",
    "\n",
    "generated_sentences = generate_sentence_from_image(\n",
    "                  model, \n",
    "                encoder_outputs, \n",
    "                tokenizer, \n",
    "                MAX_TEXT_LENGTH,\n",
    "                \"cpu\"\n",
    "            )\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
